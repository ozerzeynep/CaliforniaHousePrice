# -*- coding: utf-8 -*-
"""KesifselVeriAnaliziMakale.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1vUc8H85CysU8V9pDzBPn3AKYlKl5yR_H

# ***GEREKLİ KÜTÜPHANELERİ VE VERİ SETİNİN SAYFAYA DAHİL EDİLMESİ***
"""

!pip install shap

import numpy as np
import pandas as pd
import seaborn as sns
import matplotlib.pyplot as plt
from mpl_toolkits.mplot3d import Axes3D
from sklearn.model_selection import train_test_split
from sklearn.linear_model import LinearRegression
from sklearn.model_selection import GridSearchCV
from sklearn.ensemble import RandomForestRegressor
from sklearn.tree import DecisionTreeRegressor
from sklearn.linear_model import Lasso
from sklearn.impute import SimpleImputer
import time
import shap
from sklearn.preprocessing import StandardScaler
from sklearn.metrics import mean_squared_error, mean_absolute_error, mean_absolute_percentage_error, r2_score
import warnings
warnings.filterwarnings("ignore")

dfC = pd.read_excel("California House Price.xlsx")

"""## ***SENARYO 1 : EKSİK VERİ SİLİNECEK / STANDARDİZASYON YAPILMAYACAK / AYKIRI GÖZLEM ANALİZİ YAPILMAYACAK***"""

df = dfC.copy()

df.head(3)

df.tail(3)

df.isnull().sum()

df.dropna(inplace=True)

df.isnull().sum()

df.shape

df.info()

df.describe().T

plt.figure(figsize=(10,6))
sns.barplot(x='ocean_proximity', y='median_house_value', data=df, palette='coolwarm')
plt.xlabel("Okyanus Yakınlığı", color = "red")
plt.ylabel("Medyan Konut Fiyatı", color = "red")
plt.show()

plt.figure(figsize=(10, 6))
sns.violinplot(x='ocean_proximity', y='median_house_value', data=df, palette="Set2")
plt.xlabel("Okyanus Yakınlığı", color = "red")
plt.ylabel("Medyan Konut Fiyatı", color = "red")
plt.show()

correlation = df.select_dtypes(include=[float, int])

corr = correlation.corr()

print(corr)

plt.figure(figsize=(17,6))
sns.heatmap(corr, annot=True, cmap="coolwarm", fmt=".2f", linewidths=0.5, linecolor="red")
plt.tight_layout()
plt.show()

plt.figure(figsize=(10, 6))
plt.scatter(df['longitude'], df['latitude'], c=df['median_house_value'], cmap='viridis', marker='o', edgecolor='k', alpha=0.5)
plt.colorbar(label='Medyan Konut Fiyatı')
plt.xlabel('Boylam', color = "red")
plt.ylabel('Enlem', color = "red")
plt.show()

plt.figure(figsize=(10, 6))
plt.scatter(df['longitude'], df['latitude'], s=df['population']/100, c=df['median_house_value'], cmap='coolwarm', marker='o', edgecolor='k', alpha=0.6)
plt.colorbar(label='Medyan Konut Fiyatı')
plt.xlabel('Boylam', color = "red")
plt.ylabel('Enlem', color = "red")
plt.show()                                       #nüfusa göre medyan konut fiyatı

plt.figure(figsize=(8, 6))
sns.boxplot(x='ocean_proximity', y='median_house_value', data=df, color = "red")
plt.xlabel("Okyanus Yakınlığı", color = "red")
plt.ylabel("Medyan Konut Fiyatı", color = "red")
plt.show()

sns.pairplot(df[['median_income', 'total_rooms', 'population', 'median_house_value']], diag_kind='kde', palette='coolwarm')
plt.tight_layout()
plt.show()                #daha önemli değişkenler ile subset oluşturuldu

fig = plt.figure(figsize=(10,8))
ax = fig.add_subplot(111, projection='3d')
scatter = ax.scatter(df['households'], df['median_house_value'], c=df['median_house_value'], cmap='coolwarm', alpha=0.6)
ax.set_xlabel('Medyan Gelir', color = "red")
ax.set_ylabel('Hane Sayısı', color = "red")
ax.set_zlabel('Medyan Konut Fiyatı', color = "red")
fig.colorbar(scatter, ax=ax, shrink=0.5, aspect=5)
plt.show()                                     #hane sayısı ve konut fiyatı

y = df[["median_house_value"]]
x = df.drop(["median_house_value", "ocean_proximity"], axis = 1)

x_train, x_test, y_train, y_test = train_test_split(x,y, random_state= 42, train_size=0.70)

param_grid = {
    'fit_intercept': [True, False]
}
                                                                                              #Senaryo 1 : Linear Regression
lr = LinearRegression()
grid_search = GridSearchCV(estimator=lr, param_grid=param_grid, cv=5, n_jobs=-1)
start_train_time = time.time()
grid_search.fit(x_train, y_train)
end_train_time = time.time()
total_train_time = end_train_time - start_train_time

best_linear = grid_search.best_estimator_

y_train_pred = best_linear.predict(x_train)
train_mse = mean_squared_error(y_train, y_train_pred)
train_mae = mean_absolute_error(y_train,y_train_pred)
train_mape = mean_absolute_percentage_error(y_train, y_train_pred)
train_r2 = r2_score(y_train, y_train_pred)

start_test_time = time.time()
y_test_pred = best_linear.predict(x_test)
end_test_time = time.time()
total_test_time = end_test_time - start_test_time

test_mse = mean_squared_error(y_test, y_test_pred)
test_mae = mean_absolute_error(y_test, y_test_pred)
test_mape = mean_absolute_percentage_error(y_test, y_test_pred)
test_r2 = r2_score(y_test, y_test_pred)

print(f"Train Time: {total_train_time}")
print(f"Train MSE: {train_mse}")
print(f"Train MAE: {train_mae}")
print(f"Train MAPE: {train_mape}")
print(f"Train R2: {train_r2}")
print("---------------------------------------------")
print(f"Test Time: {total_test_time}")
print(f"Test MSE: {test_mse}")
print(f"Test MAE: {test_mae}")
print(f"Test MAPE: {test_mape}")
print(f"Test R2: {test_r2}")
print("-----------------------------------------------")
print(f"Best parameters: {grid_search.best_params_}")

explainer = shap.Explainer(best_linear, x_train)
shap_values = explainer(x_test)

shap.summary_plot(shap_values, x_test, feature_names=x_test.columns)

param_grid = {
    'max_depth': [3, 5, 8],
    'min_samples_split': [3, 5, 8],
    'min_samples_leaf': [3, 5, 8]
}
                                                                                                      #Senaryo 1 : Decision Tree Regression
dc = DecisionTreeRegressor()
grid_search = GridSearchCV(estimator=dc, param_grid = param_grid, cv= 5, n_jobs= -1)
start_train_time = time.time()
grid_search.fit(x_train, y_train)
end_train_time = time.time()
total_train_time = end_train_time - start_train_time

best_decision = grid_search.best_estimator_

y_train_pred = best_decision.predict(x_train)
train_mae = mean_absolute_error(y_train, y_train_pred)
train_mse = mean_squared_error(y_train, y_train_pred)
train_mape = mean_absolute_percentage_error(y_train, y_train_pred)
train_r2 = r2_score(y_train, y_train_pred)

start_test_time = time.time()
y_test_pred = best_decision.predict(x_test)
end_test_time = time.time()
total_test_time = end_test_time - start_test_time

test_mae = mean_absolute_error(y_test, y_test_pred)
test_mse = mean_squared_error(y_test, y_test_pred)
test_mape = mean_absolute_percentage_error(y_test, y_test_pred)
test_r2 = r2_score(y_test, y_test_pred)

print(f"Train Time : {total_train_time}")
print(f"Train MSE : {train_mse}")
print(f"Train MAE : {train_mae}")
print(f"Train MAPE : {train_mape}")
print(f"Train R2 : {train_r2}")
print("-----------------------------------")
print(f"Test Time : {total_test_time}")
print(f"Test MSE : {test_mse}")
print(f"Test MAE : {test_mae}")
print(f"Test MAPE : {test_mape}")
print(f"Test R2 : {test_r2}")
print("-------------------------------------")
print(f"Best Parametre : {grid_search.best_params_}")


explainer = shap.TreeExplainer(best_decision)
shap_values = explainer.shap_values(x_test)
shap.summary_plot(shap_values, x_test, feature_names=x_test.columns)

param_grid = {
    'n_estimators' : [20, 50],
    'max_depth' : [4, 7, 9],
    'min_samples_split' : [5, 7, 10]
}                                                                                                   #Senaryo 1 : Random Forest Regressor

rm = RandomForestRegressor()
grid_search = GridSearchCV(estimator=rm, param_grid=param_grid, cv = 5, n_jobs=-1)
start_train_time = time.time()
grid_search.fit(x_train, y_train)
end_train_time = time.time()
total_train_time = end_train_time - start_train_time

best_random = grid_search.best_estimator_

y_train_pred = best_random.predict(x_train)
train_mae = mean_absolute_error(y_train, y_train_pred)
train_mse = mean_squared_error(y_train, y_train_pred)
train_mape = mean_absolute_percentage_error(y_train, y_train_pred)
train_r2 = r2_score(y_train, y_train_pred)

start_test_time = time.time()
y_test_pred = best_random.predict(x_test)
end_test_time = time.time()
total_test_time = end_test_time - start_test_time

test_mae = mean_absolute_error(y_test, y_test_pred)
test_mse = mean_squared_error(y_test, y_test_pred)
test_mape = mean_absolute_percentage_error(y_test, y_test_pred)
test_r2 = r2_score(y_test, y_test_pred)

print(f"Train Time : {total_train_time}")
print(f"Train MSE : {train_mse}")
print(f"Train MAE : {train_mae}")
print(f"Train MAPE : {train_mape}")
print(f"Train R2 : {train_r2}")
print("-------------------------------------------")
print(f"Test Time : {total_test_time}")
print(f"Test MSE : {test_mse}")
print(f"Test MAE : {test_mae}")
print(f"Test MAPE : {test_mape}")
print(f"Test R2 : {test_r2}")
print("---------------------------------------------")
print(f"Best Parametre : {grid_search.best_params_}")

explainer = shap.Explainer(best_random)
shap_values = explainer(x_test)
shap.summary_plot(shap_values, x_test, feature_names=x_test.columns)

"""# **SENARYO 2 : EKSİK VERİ SİLİNECEK / STANDARDİZASYON YAPILACAK / AYKIRI GÖZLEM ANALİZİ YAPILMAYACAK**"""

df1 = dfC.copy()

y = df1[["median_house_value"]]
x = df1.drop(["median_house_value", "ocean_proximity"], axis = 1)

standart = StandardScaler()

x_train_scaler = standart.fit_transform(x_train)
x_test_scaler = standart.transform(x_test)

param_grid = {
    'fit_intercept': [True, False]
}
                                                                                              #Senaryo 2 : Linear Regression
lr = LinearRegression()
grid_search = GridSearchCV(estimator=lr, param_grid=param_grid, cv=5, n_jobs=-1)
start_train_time = time.time()
grid_search.fit(x_train_scaler, y_train)
end_train_time = time.time()
total_train_time = end_train_time - start_train_time

best_linear = grid_search.best_estimator_

y_train_pred = best_linear.predict(x_train_scaler)
train_mse = mean_squared_error(y_train, y_train_pred)
train_mae = mean_absolute_error(y_train,y_train_pred)
train_mape = mean_absolute_percentage_error(y_train, y_train_pred)
train_r2 = r2_score(y_train, y_train_pred)

start_test_time = time.time()
y_test_pred = best_linear.predict(x_test_scaler)
end_test_time = time.time()
total_test_time = end_test_time - start_test_time

test_mse = mean_squared_error(y_test, y_test_pred)
test_mae = mean_absolute_error(y_test, y_test_pred)
test_mape = mean_absolute_percentage_error(y_test, y_test_pred)
test_r2 = r2_score(y_test, y_test_pred)

print(f"Train Time: {total_train_time}")
print(f"Train MSE: {train_mse}")
print(f"Train MAE: {train_mae}")
print(f"Train MAPE: {train_mape}")
print(f"Train R2: {train_r2}")
print("---------------------------------------------")
print(f"Test Time: {total_test_time}")
print(f"Test MSE: {test_mse}")
print(f"Test MAE: {test_mae}")
print(f"Test MAPE: {test_mape}")
print(f"Test R2: {test_r2}")
print("-----------------------------------------------")
print(f"Best parameters: {grid_search.best_params_}")

explainer = shap.LinearExplainer(best_linear, x_train_scaler)
shap_values = explainer(x_test_scaler)
shap.summary_plot(shap_values, x_test_scaler, feature_names=x_test.columns)

param_grid = {
    'max_depth': [3, 5, 8],
    'min_samples_split': [3, 5, 8],
    'min_samples_leaf': [3, 5, 8]
}
                                                                                                      #Senaryo 2 : Decision Tree Regression
dc = DecisionTreeRegressor()
grid_search = GridSearchCV(estimator=dc, param_grid = param_grid, cv= 5, n_jobs= -1)
start_train_time = time.time()
grid_search.fit(x_train_scaler, y_train)
end_train_time = time.time()
total_train_time = end_train_time - start_train_time

best_decision = grid_search.best_estimator_

y_train_pred = best_decision.predict(x_train_scaler)
train_mae = mean_absolute_error(y_train, y_train_pred)
train_mse = mean_squared_error(y_train, y_train_pred)
train_mape = mean_absolute_percentage_error(y_train, y_train_pred)
train_r2 = r2_score(y_train, y_train_pred)

start_test_time = time.time()
y_test_pred = best_decision.predict(x_test_scaler)
end_test_time = time.time()
total_test_time = end_test_time - start_test_time

test_mae = mean_absolute_error(y_test, y_test_pred)
test_mse = mean_squared_error(y_test, y_test_pred)
test_mape = mean_absolute_percentage_error(y_test, y_test_pred)
test_r2 = r2_score(y_test, y_test_pred)

print(f"Train Time : {total_train_time}")
print(f"Train MSE : {train_mse}")
print(f"Train MAE : {train_mae}")
print(f"Train MAPE : {train_mape}")
print(f"Train R2 : {train_r2}")
print("-----------------------------------")
print(f"Test Time : {total_test_time}")
print(f"Test MSE : {test_mse}")
print(f"Test MAE : {test_mae}")
print(f"Test MAPE : {test_mape}")
print(f"Test R2 : {test_r2}")
print("-------------------------------------")
print(f"Best Parametre : {grid_search.best_params_}")


explainer = shap.TreeExplainer(best_decision)
shap_values = explainer.shap_values(x_test_scaler)
shap.summary_plot(shap_values, x_test_scaler, feature_names=x_test.columns)

param_grid = {
    'n_estimators' : [20, 50],
    'max_depth' : [4, 7, 9],
    'min_samples_split' : [5, 7, 10]
}                                                                                                   #Senaryo 2 : Random Forest Regressor

rm = RandomForestRegressor()
grid_search = GridSearchCV(estimator=rm, param_grid=param_grid, cv = 5, n_jobs=-1)
start_train_time = time.time()
grid_search.fit(x_train_scaler, y_train)
end_train_time = time.time()
total_train_time = end_train_time - start_train_time

best_random = grid_search.best_estimator_

y_train_pred = best_random.predict(x_train_scaler)
train_mae = mean_absolute_error(y_train, y_train_pred)
train_mse = mean_squared_error(y_train, y_train_pred)
train_mape = mean_absolute_percentage_error(y_train, y_train_pred)
train_r2 = r2_score(y_train, y_train_pred)

start_test_time = time.time()
y_test_pred = best_random.predict(x_test_scaler)
end_test_time = time.time()
total_test_time = end_test_time - start_test_time

test_mae = mean_absolute_error(y_test, y_test_pred)
test_mse = mean_squared_error(y_test, y_test_pred)
test_mape = mean_absolute_percentage_error(y_test, y_test_pred)
test_r2 = r2_score(y_test, y_test_pred)

print(f"Train Time : {total_train_time}")
print(f"Train MSE : {train_mse}")
print(f"Train MAE : {train_mae}")
print(f"Train MAPE : {train_mape}")
print(f"Train R2 : {train_r2}")
print("-------------------------------------------")
print(f"Test Time : {total_test_time}")
print(f"Test MSE : {test_mse}")
print(f"Test MAE : {test_mae}")
print(f"Test MAPE : {test_mape}")
print(f"Test R2 : {test_r2}")
print("---------------------------------------------")
print(f"Best Parametre : {grid_search.best_params_}")

explainer = shap.Explainer(best_random)
shap_values = explainer(x_test_scaler)
shap.summary_plot(shap_values, x_test_scaler, feature_names=x_test.columns)

"""# **SENARYO 3 : EKSİK VERİ SİLİNECEK / STANDARDİZASYON YAPILACAK / AYKIRI GÖZLEM ANALİZİNİN BASKILAM YÖNTEMİ KULLANILARAK DOLDURULMASI**"""

df2 = dfC.copy()

df2.isnull().sum()

df2.dropna(inplace=True)

df2.isnull().sum()

numeric_columns = df2.select_dtypes(include=[np.number])

q1 = numeric_columns.quantile(0.25)
q3 = numeric_columns.quantile(0.75)
IQR = q3 - q1                                                            #aykırı değerleri baskılama yöntemi ile dolduruldu

alt_sinir = q1 - 1.5 * IQR
ust_sinir = q3 + 1.5 * IQR

for column in numeric_columns.columns:
    df2[column] = np.where(df2[column] < alt_sinir[column], alt_sinir[column], df2[column])
    df2[column] = np.where(df2[column] > ust_sinir[column], ust_sinir[column], df2[column])

print("Aykırı değerler baskılama yöntemi ile sınırlandırıldıktan sonra kalan veri seti boyutu: ", df5.shape)

y = df2[["median_house_value"]]
x = df2.drop(["median_house_value", "ocean_proximity"], axis=1)

x_train, x_test, y_train, y_test = train_test_split(x, y, train_size = 0.70, random_state=42)                 #Senaryo 3 : Linear Model

standart = StandardScaler()
x_train_scaler = standart.fit_transform(x_train)
x_test_scaler = standart.transform(x_test)

param_grid = {
    'fit_intercept': [True, False]
}

lr = LinearRegression()
grid_search = GridSearchCV(estimator=lr, param_grid=param_grid, cv=5, n_jobs=-1)

start_train_time = time.time()
grid_search.fit(x_train_scaler, y_train)
end_train_time = time.time()
total_train_time = end_train_time - start_train_time

best_linear = grid_search.best_estimator_

y_train_pred = best_linear.predict(x_train_scaler)
train_mse = mean_squared_error(y_train, y_train_pred)
train_mae = mean_absolute_error(y_train, y_train_pred)
train_mape = mean_absolute_percentage_error(y_train, y_train_pred)
train_r2 = r2_score(y_train, y_train_pred)

start_test_time = time.time()
y_test_pred = best_linear.predict(x_test_scaler)
end_test_time = time.time()
total_test_time = end_test_time - start_test_time

test_mse = mean_squared_error(y_test, y_test_pred)
test_mae = mean_absolute_error(y_test, y_test_pred)
test_mape = mean_absolute_percentage_error(y_test, y_test_pred)
test_r2 = r2_score(y_test, y_test_pred)

print(f"Train Time: {total_train_time}")
print(f"Train MSE: {train_mse}")
print(f"Train MAE: {train_mae}")
print(f"Train MAPE: {train_mape}")
print(f"Train R2: {train_r2}")
print("---------------------------------------------")
print(f"Test Time: {total_test_time}")
print(f"Test MSE: {test_mse}")
print(f"Test MAE: {test_mae}")
print(f"Test MAPE: {test_mape}")
print(f"Test R2: {test_r2}")
print("-----------------------------------------------")
print(f"Best parameters: {grid_search.best_params_}")

explainer = shap.LinearExplainer(best_linear, x_train_scaler)
shap_values = explainer(x_test_scaler)

shap.summary_plot(shap_values, x_test_scaler, feature_names=x.columns)

y = df2[["median_house_value"]]
x = df2.drop(["median_house_value", "ocean_proximity"], axis=1)

x_train, x_test, y_train, y_test = train_test_split(x, y, train_size = 0.70, random_state=42)                 #Senaryo 3 : Decision Tree Regression

standart = StandardScaler()
x_train_scaler = standart.fit_transform(x_train)
x_test_scaler = standart.transform(x_test)

param_grid = {
    'max_depth': [3, 5, 8],
    'min_samples_split': [3, 5, 8],
    'min_samples_leaf': [3, 5, 8]
}

dc = DecisionTreeRegressor()
grid_search = GridSearchCV(estimator=dc, param_grid=param_grid, cv=5, n_jobs=-1)

start_train_time = time.time()
grid_search.fit(x_train_scaler, y_train)
end_train_time = time.time()
total_train_time = end_train_time - start_train_time

best_decision = grid_search.best_estimator_

y_train_pred = best_decision.predict(x_train_scaler)
train_mse = mean_squared_error(y_train, y_train_pred)
train_mae = mean_absolute_error(y_train, y_train_pred)
train_mape = mean_absolute_percentage_error(y_train, y_train_pred)
train_r2 = r2_score(y_train, y_train_pred)

start_test_time = time.time()
y_test_pred = best_decision.predict(x_test_scaler)
end_test_time = time.time()
total_test_time = end_test_time - start_test_time

test_mse = mean_squared_error(y_test, y_test_pred)
test_mae = mean_absolute_error(y_test, y_test_pred)
test_mape = mean_absolute_percentage_error(y_test, y_test_pred)
test_r2 = r2_score(y_test, y_test_pred)

print(f"Train Time: {total_train_time}")
print(f"Train MSE: {train_mse}")
print(f"Train MAE: {train_mae}")
print(f"Train MAPE: {train_mape}")
print(f"Train R2: {train_r2}")
print("---------------------------------------------")
print(f"Test Time: {total_test_time}")
print(f"Test MSE: {test_mse}")
print(f"Test MAE: {test_mae}")
print(f"Test MAPE: {test_mape}")
print(f"Test R2: {test_r2}")
print("-----------------------------------------------")
print(f"Best parameters: {grid_search.best_params_}")

explainer = shap.TreeExplainer(best_decision)
shap_values = explainer(x_test_scaler)

shap.summary_plot(shap_values, x_test_scaler, feature_names=x.columns)

y = df2[["median_house_value"]]
x = df2.drop(["median_house_value", "ocean_proximity"], axis=1)

x_train, x_test, y_train, y_test = train_test_split(x, y, train_size = 0.70, random_state=42)                 #Senaryo 3 : Random Forest Regression

standart = StandardScaler()
x_train_scaler = standart.fit_transform(x_train)
x_test_scaler = standart.transform(x_test)

param_grid = {
    'n_estimators' : [20, 50],
    'max_depth' : [4, 7, 9],
    'min_samples_split' : [5, 7, 10]
}

rm = RandomForestRegressor()
grid_search = GridSearchCV(estimator=rm, param_grid=param_grid, cv=5, n_jobs=-1)

start_train_time = time.time()
grid_search.fit(x_train_scaler, y_train)
end_train_time = time.time()
total_train_time = end_train_time - start_train_time

best_random = grid_search.best_estimator_

y_train_pred = best_random.predict(x_train_scaler)
train_mse = mean_squared_error(y_train, y_train_pred)
train_mae = mean_absolute_error(y_train, y_train_pred)
train_mape = mean_absolute_percentage_error(y_train, y_train_pred)
train_r2 = r2_score(y_train, y_train_pred)

start_test_time = time.time()
y_test_pred = best_random.predict(x_test_scaler)
end_test_time = time.time()
total_test_time = end_test_time - start_test_time

test_mse = mean_squared_error(y_test, y_test_pred)
test_mae = mean_absolute_error(y_test, y_test_pred)
test_mape = mean_absolute_percentage_error(y_test, y_test_pred)
test_r2 = r2_score(y_test, y_test_pred)

print(f"Train Time: {total_train_time}")
print(f"Train MSE: {train_mse}")
print(f"Train MAE: {train_mae}")
print(f"Train MAPE: {train_mape}")
print(f"Train R2: {train_r2}")
print("---------------------------------------------")
print(f"Test Time: {total_test_time}")
print(f"Test MSE: {test_mse}")
print(f"Test MAE: {test_mae}")
print(f"Test MAPE: {test_mape}")
print(f"Test R2: {test_r2}")
print("-----------------------------------------------")
print(f"Best parameters: {grid_search.best_params_}")

explainer = shap.Explainer(best_random)
shap_values = explainer(x_test_scaler)

shap.summary_plot(shap_values, x_test_scaler, feature_names=x.columns)

"""# ***SENARYO 4 : EKSİK VERiLERİ ORTALAMA İLE DOLDURMA / STANDARDİZASYON YAPILMAYACAK / AYKIRI GÖZLEM ANALİZİ YAPILMAYACAK***

"""

df3 = dfC.copy()

df3.isnull().sum()

df3['total_bedrooms'] = df3['total_bedrooms'].fillna(df3['total_bedrooms'].mean())

df3.isnull().sum()

df3.describe().T

df3.info()

plt.figure(figsize=(10,6))
sns.barplot(x='ocean_proximity', y='median_house_value', data=df3, palette='magma')
plt.xlabel("Okyanus Yakınlığı", color = "red")
plt.ylabel("Medyan Konut Fiyatı", color = "red")
plt.show()

plt.figure(figsize=(10, 6))
sns.violinplot(x='ocean_proximity', y='median_house_value', data=df3, palette="Set1")
plt.xlabel("Okyanus Yakınlığı", color = "red")
plt.ylabel("Medyan Konut Fiyatı", color = "red")
plt.show()

correlation = df3.select_dtypes(include=[float, int])

corr1 = correlation.corr()

print(corr1)

plt.figure(figsize=(17,6))
sns.heatmap(corr1, annot=True, cmap="plasma", fmt=".2f", linewidths=0.5, linecolor="red")
plt.tight_layout()
plt.show()

plt.figure(figsize=(10, 6))
plt.scatter(df3['longitude'], df3['latitude'], c=df3['median_house_value'], cmap='cividis', marker='o', edgecolor='k', alpha=0.5)
plt.colorbar(label='Medyan Konut Fiyatı')
plt.xlabel('Boylam', color = "red")
plt.ylabel('Enlem', color = "red")
plt.show()

plt.figure(figsize=(10, 6))
plt.scatter(df3['longitude'], df3['latitude'], s=df3['population']/100, c=df3['median_house_value'], cmap='cividis', marker='o', edgecolor='k', alpha=0.6)
plt.colorbar(label='Medyan Konut Fiyatı')
plt.xlabel('Boylam', color = "red")
plt.ylabel('Enlem', color = "red")
plt.show()

plt.figure(figsize=(8, 6))
sns.boxplot(x='ocean_proximity', y='median_house_value', data=df3, color = "yellow")
plt.xlabel("Okyanus Yakınlığı", color = "red")
plt.ylabel("Medyan Konut Fiyatı", color = "red")
plt.show()

sns.pairplot(df3[['median_income', 'total_rooms', 'population', 'median_house_value']], diag_kind='kde')
plt.tight_layout()
plt.show()

fig = plt.figure(figsize=(10,8))
ax = fig.add_subplot(111, projection='3d')
scatter = ax.scatter(df3['households'], df3['median_house_value'], c=df3['median_house_value'], cmap='cividis', alpha=0.6)
ax.set_xlabel('Medyan Gelir', color = "red")
ax.set_ylabel('Hane Sayısı', color = "red")
ax.set_zlabel('Medyan Konut Fiyatı', color = "red")
fig.colorbar(scatter, ax=ax, shrink=0.5, aspect=5)
plt.show()                                     #hane sayısı ve konut fiyatı

y = df3[["median_house_value"]]
x = df3.drop(["median_house_value", "ocean_proximity"], axis = 1)

x_train, x_test, y_train, y_test = train_test_split(x,y, train_size = 0.70, random_state=42)

param_grid = {
    'fit_intercept': [True, False]
}
                                                                                              #Senaryo 4 : Linear Regression
lr = LinearRegression()
grid_search = GridSearchCV(estimator=lr, param_grid=param_grid, cv=5, n_jobs=-1)
start_train_time = time.time()
grid_search.fit(x_train, y_train)
end_train_time = time.time()
total_train_time = end_train_time - start_train_time

best_linear = grid_search.best_estimator_

y_train_pred = best_linear.predict(x_train)
train_mse = mean_squared_error(y_train, y_train_pred)
train_mae = mean_absolute_error(y_train,y_train_pred)
train_mape = mean_absolute_percentage_error(y_train, y_train_pred)
train_r2 = r2_score(y_train, y_train_pred)

start_test_time = time.time()
y_test_pred = best_linear.predict(x_test)
end_test_time = time.time()
total_test_time = end_test_time - start_test_time

test_mse = mean_squared_error(y_test, y_test_pred)
test_mae = mean_absolute_error(y_test, y_test_pred)
test_mape = mean_absolute_percentage_error(y_test, y_test_pred)
test_r2 = r2_score(y_test, y_test_pred)

print(f"Train Time: {total_train_time}")
print(f"Train MSE: {train_mse}")
print(f"Train MAE: {train_mae}")
print(f"Train MAPE: {train_mape}")
print(f"Train R2: {train_r2}")
print("---------------------------------------------")
print(f"Test Time: {total_test_time}")
print(f"Test MSE: {test_mse}")
print(f"Test MAE: {test_mae}")
print(f"Test MAPE: {test_mape}")
print(f"Test R2: {test_r2}")
print("-----------------------------------------------")
print(f"Best parameters: {grid_search.best_params_}")

explainer = shap.Explainer(best_linear, x_train)
shap_values = explainer(x_test)

shap.summary_plot(shap_values, x_test, feature_names=x_test.columns)

param_grid = {
    'max_depth': [3, 5, 8],
    'min_samples_split': [3, 5, 8],
    'min_samples_leaf': [3, 5, 8]
}
                                                                                              #Senaryo 4 : Decision Tree Regression
dc = DecisionTreeRegressor()
grid_search = GridSearchCV(estimator=dc, param_grid=param_grid, cv=5, n_jobs=-1)
start_train_time = time.time()
grid_search.fit(x_train, y_train)
end_train_time = time.time()
total_train_time = end_train_time - start_train_time

best_decision= grid_search.best_estimator_

y_train_pred = best_decision.predict(x_train)
train_mse = mean_squared_error(y_train, y_train_pred)
train_mae = mean_absolute_error(y_train,y_train_pred)
train_mape = mean_absolute_percentage_error(y_train, y_train_pred)
train_r2 = r2_score(y_train, y_train_pred)

start_test_time = time.time()
y_test_pred = best_decision.predict(x_test)
end_test_time = time.time()
total_test_time = end_test_time - start_test_time

test_mse = mean_squared_error(y_test, y_test_pred)
test_mae = mean_absolute_error(y_test, y_test_pred)
test_mape = mean_absolute_percentage_error(y_test, y_test_pred)
test_r2 = r2_score(y_test, y_test_pred)

print(f"Train Time: {total_train_time}")
print(f"Train MSE: {train_mse}")
print(f"Train MAE: {train_mae}")
print(f"Train MAPE: {train_mape}")
print(f"Train R2: {train_r2}")
print("---------------------------------------------")
print(f"Test Time: {total_test_time}")
print(f"Test MSE: {test_mse}")
print(f"Test MAE: {test_mae}")
print(f"Test MAPE: {test_mape}")
print(f"Test R2: {test_r2}")
print("-----------------------------------------------")
print(f"Best parameters: {grid_search.best_params_}")

explainer = shap.TreeExplainer(best_decision)
shap_values = explainer.shap_values(x_test)
shap.summary_plot(shap_values, x_test, feature_names=x_test.columns)

param_grid = {
    'n_estimators' : [20, 50],
    'max_depth' : [4, 7, 9],
    'min_samples_split' : [5, 7, 10]
}
                                                                                              #Senaryo 4 : Random Forest Regression
rm = RandomForestRegressor()
grid_search = GridSearchCV(estimator=rm, param_grid=param_grid, cv=5, n_jobs=-1)
start_train_time = time.time()
grid_search.fit(x_train, y_train)
end_train_time = time.time()
total_train_time = end_train_time - start_train_time

best_random= grid_search.best_estimator_

y_train_pred = best_random.predict(x_train)
train_mse = mean_squared_error(y_train, y_train_pred)
train_mae = mean_absolute_error(y_train,y_train_pred)
train_mape = mean_absolute_percentage_error(y_train, y_train_pred)
train_r2 = r2_score(y_train, y_train_pred)

start_test_time = time.time()
y_test_pred = best_random.predict(x_test)
end_test_time = time.time()
total_test_time = end_test_time - start_test_time

test_mse = mean_squared_error(y_test, y_test_pred)
test_mae = mean_absolute_error(y_test, y_test_pred)
test_mape = mean_absolute_percentage_error(y_test, y_test_pred)
test_r2 = r2_score(y_test, y_test_pred)

print(f"Train Time: {total_train_time}")
print(f"Train MSE: {train_mse}")
print(f"Train MAE: {train_mae}")
print(f"Train MAPE: {train_mape}")
print(f"Train R2: {train_r2}")
print("---------------------------------------------")
print(f"Test Time: {total_test_time}")
print(f"Test MSE: {test_mse}")
print(f"Test MAE: {test_mae}")
print(f"Test MAPE: {test_mape}")
print(f"Test R2: {test_r2}")
print("-----------------------------------------------")
print(f"Best parameters: {grid_search.best_params_}")

explainer = shap.Explainer(best_random)
shap_values = explainer(x_test)
shap.summary_plot(shap_values, x_test, feature_names=x_test.columns)

"""# ***SENARYO 5 : EKSİK VERiLERİ ORTALAMA İLE DOLDURMA / STANDARDİZASYON YAPILACAK / AYKIRI GÖZLEM ANALİZİ YAPILMAYACAK***"""

df4 = dfC.copy()

df4.isnull().sum()

df4['total_bedrooms'] = df4['total_bedrooms'].fillna(df4['total_bedrooms'].mean())

df4.isnull().sum()

stantdart = StandardScaler()

x_train_scaler = standart.fit_transform(x_train)
x_test_scaler = standart.transform(x_test)

param_grid = {
    'fit_intercept': [True, False]
}
                                                                                              #Senaryo 5 : Linear Regression
lr = LinearRegression()
grid_search = GridSearchCV(estimator=lr, param_grid=param_grid, cv=5, n_jobs=-1)
start_train_time = time.time()
grid_search.fit(x_train_scaler, y_train)
end_train_time = time.time()
total_train_time = end_train_time - start_train_time

best_linear = grid_search.best_estimator_

y_train_pred = best_linear.predict(x_train_scaler)
train_mse = mean_squared_error(y_train, y_train_pred)
train_mae = mean_absolute_error(y_train,y_train_pred)
train_mape = mean_absolute_percentage_error(y_train, y_train_pred)
train_r2 = r2_score(y_train, y_train_pred)

start_test_time = time.time()
y_test_pred = best_linear.predict(x_test_scaler)
end_test_time = time.time()
total_test_time = end_test_time - start_test_time

test_mse = mean_squared_error(y_test, y_test_pred)
test_mae = mean_absolute_error(y_test, y_test_pred)
test_mape = mean_absolute_percentage_error(y_test, y_test_pred)
test_r2 = r2_score(y_test, y_test_pred)

print(f"Train Time: {total_train_time}")
print(f"Train MSE: {train_mse}")
print(f"Train MAE: {train_mae}")
print(f"Train MAPE: {train_mape}")
print(f"Train R2: {train_r2}")
print("---------------------------------------------")
print(f"Test Time: {total_test_time}")
print(f"Test MSE: {test_mse}")
print(f"Test MAE: {test_mae}")
print(f"Test MAPE: {test_mape}")
print(f"Test R2: {test_r2}")
print("-----------------------------------------------")
print(f"Best parameters: {grid_search.best_params_}")

explainer = shap.LinearExplainer(best_linear, x_train_scaler)
shap_values = explainer(x_test_scaler)
shap.summary_plot(shap_values, x_test_scaler, feature_names=x_test.columns)

param_grid = {
    'max_depth': [3, 5, 8],
    'min_samples_split': [3, 5, 8],
    'min_samples_leaf': [3, 5, 8]
}
                                                                                                      #Senaryo 5 : Decision Tree Regression
dc = DecisionTreeRegressor()
grid_search = GridSearchCV(estimator=dc, param_grid = param_grid, cv= 5, n_jobs= -1)
start_train_time = time.time()
grid_search.fit(x_train_scaler, y_train)
end_train_time = time.time()
total_train_time = end_train_time - start_train_time

best_decision = grid_search.best_estimator_

y_train_pred = best_decision.predict(x_train_scaler)
train_mae = mean_absolute_error(y_train, y_train_pred)
train_mse = mean_squared_error(y_train, y_train_pred)
train_mape = mean_absolute_percentage_error(y_train, y_train_pred)
train_r2 = r2_score(y_train, y_train_pred)

start_test_time = time.time()
y_test_pred = best_decision.predict(x_test_scaler)
end_test_time = time.time()
total_test_time = end_test_time - start_test_time

test_mae = mean_absolute_error(y_test, y_test_pred)
test_mse = mean_squared_error(y_test, y_test_pred)
test_mape = mean_absolute_percentage_error(y_test, y_test_pred)
test_r2 = r2_score(y_test, y_test_pred)

print(f"Train Time : {total_train_time}")
print(f"Train MSE : {train_mse}")
print(f"Train MAE : {train_mae}")
print(f"Train MAPE : {train_mape}")
print(f"Train R2 : {train_r2}")
print("-----------------------------------")
print(f"Test Time : {total_test_time}")
print(f"Test MSE : {test_mse}")
print(f"Test MAE : {test_mae}")
print(f"Test MAPE : {test_mape}")
print(f"Test R2 : {test_r2}")
print("-------------------------------------")
print(f"Best Parametre : {grid_search.best_params_}")


explainer = shap.TreeExplainer(best_decision)
shap_values = explainer.shap_values(x_test_scaler)
shap.summary_plot(shap_values, x_test_scaler, feature_names=x_test.columns)

param_grid = {
    'n_estimators' : [20, 50],
    'max_depth' : [4, 7, 9],
    'min_samples_split' : [5, 7, 10]
}                                                                                                   #Senaryo 5 : Random Forest Regressor

rm = RandomForestRegressor()
grid_search = GridSearchCV(estimator=rm, param_grid=param_grid, cv = 5, n_jobs=-1)
start_train_time = time.time()
grid_search.fit(x_train_scaler, y_train)
end_train_time = time.time()
total_train_time = end_train_time - start_train_time

best_random = grid_search.best_estimator_

y_train_pred = best_random.predict(x_train_scaler)
train_mae = mean_absolute_error(y_train, y_train_pred)
train_mse = mean_squared_error(y_train, y_train_pred)
train_mape = mean_absolute_percentage_error(y_train, y_train_pred)
train_r2 = r2_score(y_train, y_train_pred)

start_test_time = time.time()
y_test_pred = best_random.predict(x_test_scaler)
end_test_time = time.time()
total_test_time = end_test_time - start_test_time

test_mae = mean_absolute_error(y_test, y_test_pred)
test_mse = mean_squared_error(y_test, y_test_pred)
test_mape = mean_absolute_percentage_error(y_test, y_test_pred)
test_r2 = r2_score(y_test, y_test_pred)

print(f"Train Time : {total_train_time}")
print(f"Train MSE : {train_mse}")
print(f"Train MAE : {train_mae}")
print(f"Train MAPE : {train_mape}")
print(f"Train R2 : {train_r2}")
print("-------------------------------------------")
print(f"Test Time : {total_test_time}")
print(f"Test MSE : {test_mse}")
print(f"Test MAE : {test_mae}")
print(f"Test MAPE : {test_mape}")
print(f"Test R2 : {test_r2}")
print("---------------------------------------------")
print(f"Best Parametre : {grid_search.best_params_}")

explainer = shap.Explainer(best_random)
shap_values = explainer(x_test_scaler)
shap.summary_plot(shap_values, x_test_scaler, feature_names=x_test.columns)

"""# ***SENARYO 6 : EKSİK VERiLERİ ORTALAMA İLE DOLDURMA / STANDARDİZASYON YAPILACAK / AYKIRI GÖZLEM ANALİZİNİN BASKILAMA YÖNTEMİ İLE DOLDURULMASI***"""

df5 = dfC.copy()

df5.isnull().sum()

df5['total_bedrooms'] = df5['total_bedrooms'].fillna(df5['total_bedrooms'].mean())

numeric_columns = df5.select_dtypes(include=[np.number])

q1 = numeric_columns.quantile(0.25)
q3 = numeric_columns.quantile(0.75)
IQR = q3 - q1                                                            #aykırı değerleri baskılama yöntemi ile dolduruldu

alt_sinir = q1 - 1.5 * IQR
ust_sinir = q3 + 1.5 * IQR

for column in numeric_columns.columns:
    df5[column] = np.where(df5[column] < alt_sinir[column], alt_sinir[column], df5[column])
    df5[column] = np.where(df5[column] > ust_sinir[column], ust_sinir[column], df5[column])

print("Aykırı değerler baskılama yöntemi ile sınırlandırıldıktan sonra kalan veri seti boyutu: ", df5.shape)

y = df5[["median_house_value"]]
x = df5.drop(["median_house_value", "ocean_proximity"], axis=1)

x_train, x_test, y_train, y_test = train_test_split(x, y, train_size=0.70, random_state=42)

scaler = StandardScaler()
x_train_scaler = scaler.fit_transform(x_train)
x_test_scaler = scaler.transform(x_test)                                                                #Senaryo 6 : Linear Regression

param_grid = {
    'fit_intercept': [True, False]
}

lr = LinearRegression()
grid_search = GridSearchCV(estimator=lr, param_grid=param_grid, cv=5, n_jobs=-1)

start_train_time = time.time()
grid_search.fit(x_train_scaler, y_train)
end_train_time = time.time()
total_train_time = end_train_time - start_train_time

best_linear = grid_search.best_estimator_

y_train_pred = best_linear.predict(x_train_scaler)
train_mse = mean_squared_error(y_train, y_train_pred)
train_mae = mean_absolute_error(y_train, y_train_pred)
train_mape = mean_absolute_percentage_error(y_train, y_train_pred)
train_r2 = r2_score(y_train, y_train_pred)

start_test_time = time.time()
y_test_pred = best_linear.predict(x_test_scaler)
end_test_time = time.time()
total_test_time = end_test_time - start_test_time

test_mse = mean_squared_error(y_test, y_test_pred)
test_mae = mean_absolute_error(y_test, y_test_pred)
test_mape = mean_absolute_percentage_error(y_test, y_test_pred)
test_r2 = r2_score(y_test, y_test_pred)

print(f"Train Time: {total_train_time}")
print(f"Train MSE: {train_mse}")
print(f"Train MAE: {train_mae}")
print(f"Train MAPE: {train_mape}")
print(f"Train R2: {train_r2}")
print("---------------------------------------------")
print(f"Test Time: {total_test_time}")
print(f"Test MSE: {test_mse}")
print(f"Test MAE: {test_mae}")
print(f"Test MAPE: {test_mape}")
print(f"Test R2: {test_r2}")
print("-----------------------------------------------")
print(f"Best parameters: {grid_search.best_params_}")


explainer = shap.LinearExplainer(best_linear, x_train_scaler)
shap_values = explainer(x_test_scaler)
shap.summary_plot(shap_values, x_test_scaler, feature_names=x.columns)

y = df5[["median_house_value"]]
x = df5.drop(["median_house_value", "ocean_proximity"], axis=1)

x_train, x_test, y_train, y_test = train_test_split(x, y, train_size = 0.70, random_state=42)                 #Senaryo 6 : Decision Tree Regression

standart = StandardScaler()
x_train_scaler = standart.fit_transform(x_train)
x_test_scaler = standart.transform(x_test)

param_grid = {
    'max_depth': [3, 5, 8],
    'min_samples_split': [3, 5, 8],
    'min_samples_leaf': [3, 5, 8]
}

dc = DecisionTreeRegressor()
grid_search = GridSearchCV(estimator=dc, param_grid=param_grid, cv=5, n_jobs=-1)

start_train_time = time.time()
grid_search.fit(x_train_scaler, y_train)
end_train_time = time.time()
total_train_time = end_train_time - start_train_time

best_decision = grid_search.best_estimator_

y_train_pred = best_decision.predict(x_train_scaler)
train_mse = mean_squared_error(y_train, y_train_pred)
train_mae = mean_absolute_error(y_train, y_train_pred)
train_mape = mean_absolute_percentage_error(y_train, y_train_pred)
train_r2 = r2_score(y_train, y_train_pred)

start_test_time = time.time()
y_test_pred = best_decision.predict(x_test_scaler)
end_test_time = time.time()
total_test_time = end_test_time - start_test_time

test_mse = mean_squared_error(y_test, y_test_pred)
test_mae = mean_absolute_error(y_test, y_test_pred)
test_mape = mean_absolute_percentage_error(y_test, y_test_pred)
test_r2 = r2_score(y_test, y_test_pred)

print(f"Train Time: {total_train_time}")
print(f"Train MSE: {train_mse}")
print(f"Train MAE: {train_mae}")
print(f"Train MAPE: {train_mape}")
print(f"Train R2: {train_r2}")
print("---------------------------------------------")
print(f"Test Time: {total_test_time}")
print(f"Test MSE: {test_mse}")
print(f"Test MAE: {test_mae}")
print(f"Test MAPE: {test_mape}")
print(f"Test R2: {test_r2}")
print("-----------------------------------------------")
print(f"Best parameters: {grid_search.best_params_}")

explainer = shap.TreeExplainer(best_decision)
shap_values = explainer(x_test_scaler)
shap.summary_plot(shap_values, x_test_scaler, feature_names=x.columns)

y = df5[["median_house_value"]]
x = df5.drop(["median_house_value", "ocean_proximity"], axis=1)

x_train, x_test, y_train, y_test = train_test_split(x, y, train_size = 0.70, random_state=42)                 #Senaryo 6 : Random Forest Regression

standart = StandardScaler()
x_train_scaler = standart.fit_transform(x_train)
x_test_scaler = standart.transform(x_test)

param_grid = {
    'n_estimators' : [20, 50],
    'max_depth' : [4, 7, 9],
    'min_samples_split' : [5, 7, 10]
}

rm = RandomForestRegressor()
grid_search = GridSearchCV(estimator=rm, param_grid=param_grid, cv=5, n_jobs=-1)

start_train_time = time.time()
grid_search.fit(x_train_scaler, y_train)
end_train_time = time.time()
total_train_time = end_train_time - start_train_time

best_random = grid_search.best_estimator_

y_train_pred = best_random.predict(x_train_scaler)
train_mse = mean_squared_error(y_train, y_train_pred)
train_mae = mean_absolute_error(y_train, y_train_pred)
train_mape = mean_absolute_percentage_error(y_train, y_train_pred)
train_r2 = r2_score(y_train, y_train_pred)

start_test_time = time.time()
y_test_pred = best_random.predict(x_test_scaler)
end_test_time = time.time()
total_test_time = end_test_time - start_test_time

test_mse = mean_squared_error(y_test, y_test_pred)
test_mae = mean_absolute_error(y_test, y_test_pred)
test_mape = mean_absolute_percentage_error(y_test, y_test_pred)
test_r2 = r2_score(y_test, y_test_pred)

print(f"Train Time: {total_train_time}")
print(f"Train MSE: {train_mse}")
print(f"Train MAE: {train_mae}")
print(f"Train MAPE: {train_mape}")
print(f"Train R2: {train_r2}")
print("---------------------------------------------")
print(f"Test Time: {total_test_time}")
print(f"Test MSE: {test_mse}")
print(f"Test MAE: {test_mae}")
print(f"Test MAPE: {test_mape}")
print(f"Test R2: {test_r2}")
print("-----------------------------------------------")
print(f"Best parameters: {grid_search.best_params_}")

explainer = shap.Explainer(best_random)
shap_values = explainer(x_test_scaler)

shap.summary_plot(shap_values, x_test_scaler, feature_names=x.columns)

"""# ***SENARYO 7 : VERİ SETİ SAYFAYA DAHİL EDİLDİKTEN SONRA KEŞİFSEL VERİ ANALİZİ HİÇ UYGULANMADAN MODELLEMEYE GEÇİLMESİ***"""

df7 = dfC.copy()

y = df7[["median_house_value"]]
x = df7.drop(["median_house_value", "ocean_proximity"], axis=1)

x_train, x_test, y_train, y_test = train_test_split(x,y, train_size = 0.70, random_state = 42)

param_grid = {
    'fit_intercept': [True, False]
}                                                                                                    #Senaryo 7 : Linear Regression


lr = LinearRegression()
grid_search = GridSearchCV(estimator=lr, param_grid=param_grid, cv=5, n_jobs=-1)

start_train_time = time.time()
grid_search.fit(x_train, y_train)
end_train_time = time.time()
total_train_time = end_train_time - start_train_time

best_linear = grid_search.best_estimator_

y_train_pred = best_linear.predict(x_train)
train_mse = mean_squared_error(y_train, y_train_pred)
train_mae = mean_absolute_error(y_train, y_train_pred)
train_mape = mean_absolute_percentage_error(y_train, y_train_pred)
train_r2 = r2_score(y_train, y_train_pred)

start_test_time = time.time()
y_test_pred = best_linear.predict(x_test)
end_test_time = time.time()
total_test_time = end_test_time - start_test_time

test_mse = mean_squared_error(y_test, y_test_pred)
test_mae = mean_absolute_error(y_test, y_test_pred)
test_mape = mean_absolute_percentage_error(y_test, y_test_pred)
test_r2 = r2_score(y_test, y_test_pred)

print(f"Train Time: {total_train_time}")
print(f"Train MSE: {train_mse}")
print(f"Train MAE: {train_mae}")
print(f"Train MAPE: {train_mape}")
print(f"Train R2: {train_r2}")
print("---------------------------------------------")
print(f"Test Time: {total_test_time}")
print(f"Test MSE: {test_mse}")
print(f"Test MAE: {test_mae}")
print(f"Test MAPE: {test_mape}")
print(f"Test R2: {test_r2}")
print("-----------------------------------------------")
print(f"Best parameters: {grid_search.best_params_}")


explainer = shap.LinearExplainer(best_linear, x_train_scaler)
shap_values = explainer(x_test_scaler)
shap.summary_plot(shap_values, x_test_scaler, feature_names=x.columns)

"""Burada keşifsel veri analizi yapılmadığından veri seti içinden boş değerin olduğunu bilemiyorum ve model boş değer olduğu için eğitimini tamamlayamıyor."""

